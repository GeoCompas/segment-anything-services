{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending a request to the torchserve encoder service for the Segment Anything model (SAM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running services locally, make sure to select the correct port. 70* for the cpu service, 80* for the gpu service.\n",
    "\n",
    "Note, the GPU service doesn't support the decoder model since this can be run on the CPU service!\n",
    "\n",
    "The CPU service supports the encoder (slow) and the decoder (fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import httpx\n",
    "import base64\n",
    "from PIL import Image\n",
    "from PIL.ImageOps import autocontrast\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# default localhost endpoints after starting both containers, see README\n",
    "# encode_url=\"http://127.0.0.1:7080/predictions/sam_vit_h_encode\"\n",
    "encode_url=\"http://127.0.0.1:8080/predictions/sam_vit_h_encode\"\n",
    "\n",
    "pth_slick = \"../data/tile_with_slick_512_512.png\"\n",
    "input_point_not_on_slick = (10, 120)\n",
    "input_point_on_slick = (6, 120)\n",
    "input_label = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run SAM on a small subset of a Sentinel-1 image that captured an oil slick on the ocean from a shipping vessel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_slick = Image.open(pth_slick)\n",
    "autocontrast(img_slick, cutoff=0, ignore=None, mask=None, preserve_tone=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads image as bytes, converts bytes to string so it can be sent as a post request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(pth_slick, 'rb') as f:\n",
    "    byte_string = f.read()\n",
    "    base64_string = base64.b64encode(byte_string).decode('utf-8')\n",
    "\n",
    "payload = {\"encoded_image\": base64_string}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's run the image encoder locally. Use the CPU endpoint if you don't have a GPU. Timings will differ based on the GPU type or if running on the CPU it will take over a minute for an unoptimized model. Time to encode image on 1080 Ti GPU is about 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    response = httpx.post(encode_url, json=payload, timeout=None)\n",
    "except (BrokenPipeError, httpx.RemoteProtocolError, ConnectionResetError) as e:\n",
    "    print(\"wait and try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the image embeddings for the oil slick scene and converting to a numpy array. The image embeddings represent the features of the image that we can produce mask predictions from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_embedding_string = response.json()['image_embedding']\n",
    "base64_bytes = base64.b64decode(encoded_embedding_string)\n",
    "image_embedding = np.frombuffer(base64_bytes, dtype=np.float32)\n",
    "image_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we send the image embeddings to the decoder service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_shape = np.array(img_slick).shape\n",
    "decode_payload = {\n",
    "    \"image_embeddings\": encoded_embedding_string,\n",
    "    \"image_shape\": img_shape,\n",
    "    \"input_label\": input_label,\n",
    "    \"input_point\": input_point_on_slick\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "decode_url=\"http://127.0.0.1:7080/predictions/sam_vit_h_decode\" # make sure to select correct port. 70* for cpu, 80* for gpu\n",
    "response = httpx.post(decode_url, json=decode_payload, timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_masks_string = response.json()['masks']\n",
    "base64_bytes_masks = base64.b64decode(encoded_masks_string)\n",
    "masks = np.frombuffer(base64_bytes_masks, dtype=bool)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four masks, each with their own confidence score, predicted for the single point prompt. SAM makes an effort to predict valid masks in cases where there is ambiguity as to which object is desired and minimal prompting. See the SAM paper https://arxiv.org/pdf/2304.02643.pdf for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "masks = masks.reshape((1,4,512, 512))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax):\n",
    "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our encoder and decoder service, we get a solid mask prediction by just supplying a point on the object of interest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.array(img_slick))\n",
    "show_mask(masks[0,1,:,:], plt.gca())\n",
    "input_point_arr = np.array(input_point_on_slick)[np.newaxis,:]\n",
    "input_label_arr = np.array(input_label)[np.newaxis]\n",
    "show_points(input_point_arr, input_label_arr, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's test the geospatial endpoint. For geospatial imagery it is more useful to return a georeferenced mask instead of an unreferenced numpy array so that we can plot these predictions on a map and associate them with other geospatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import io\n",
    "from skimage import img_as_ubyte\n",
    "with rasterio.open(\"../data/sample-georeferenced_burn_scar.tif\") as dataset:\n",
    "    arr = dataset.read()\n",
    "    bbox = dataset.bounds\n",
    "    crs = \"EPSG:32610\"\n",
    "\n",
    "\n",
    "arr = img_as_ubyte(arr).transpose((1,2,0))\n",
    "\n",
    "img = Image.fromarray(arr)\n",
    "\n",
    "# Create byte stream\n",
    "buffered = io.BytesIO()\n",
    "img.save(buffered, format=\"PNG\")\n",
    "img_str = base64.b64encode(buffered.getvalue())\n",
    "base64_string = img_str.decode('utf-8')\n",
    "payload = {\"encoded_image\": base64_string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    response = httpx.post(encode_url, json=payload, timeout=None)\n",
    "except (BrokenPipeError, httpx.RemoteProtocolError) as e:\n",
    "    print(\"wait and try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_embedding_string = response.json()['image_embedding']\n",
    "base64_bytes = base64.b64decode(encoded_embedding_string)\n",
    "image_embedding = np.frombuffer(base64_bytes, dtype=np.float32)\n",
    "image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_point_on_burn = (220,120)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test SAM on a Sentinel-2 image of a burn scar in a USA agricultural region. SAM does a decent job with single point prompting of delineating the burn scar but adds in some incorrect pixels to the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.array(img))\n",
    "input_point_arr = np.array(input_point_on_burn)[np.newaxis,:]\n",
    "input_label_arr = np.array(input_label)[np.newaxis]\n",
    "show_points(input_point_arr, input_label_arr, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our source crs. The decoder service will reproject the outputs to WGS84 no matter the source CRS (the source CRS can only be supplied via epsg code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_shape = img.size\n",
    "decode_payload = {\n",
    "    \"image_embeddings\": encoded_embedding_string,\n",
    "    \"image_shape\": img_shape,\n",
    "    \"input_label\": input_label,\n",
    "    \"input_point\": input_point_on_burn,\n",
    "    \"crs\":crs,\n",
    "    \"bbox\": list(bbox),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "decode_url=\"http://127.0.0.1:7080/predictions/sam_vit_h_decode\" # make sure to select correct port. 70* for cpu, 80* for gpu\n",
    "response = httpx.post(decode_url, json=decode_payload, timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geojson_masks = response.json()['geojsons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(geojson_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(geojson_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geojson_masks[0][0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geojson_dict = json.loads(geojson_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('multi_polygon.geojson', 'w') as f:\n",
    "    f.write(geojson_masks[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(geojson_masks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the geojson result in your favorite GIS! Like the slick image, we have 4 masks, represented by 4 MultiPolygon types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
